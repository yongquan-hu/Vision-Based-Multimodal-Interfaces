
# Vision-Based Multimodal Interfaces

This repository provides the official paper, citation, and supplementary materials for our CHI 2025 paper on **Vision-Based Multimodal Interfaces (VMIs)**.

<p align="center">
  <img src="https://github.com/yongquan-hu/Vision-Based-Multimodal-Interfaces/blob/main/VMIs_title.png" width="600" alt="VMIs Teaser">
</p>

---

## üìÑ Paper Access

- **ACM Digital Library**: [https://dl.acm.org/doi/10.1145/3706598.3714161](https://dl.acm.org/doi/10.1145/3706598.3714161)  
- **arXiv**: [https://arxiv.org/abs/2501.13443](https://arxiv.org/abs/2501.13443)

---

## üìö Citation

- **Google Scholar**: [View on Google Scholar](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QqeG8WIAAAAJ&citation_for_view=QqeG8WIAAAAJ:YOwf2qJgpHMC)

### BibTeX
```bibtex
@inproceedings{hu2025vision,
  title     = {Vision-based multimodal interfaces: A survey and taxonomy for enhanced context-aware system design},
  author    = {Hu, Yongquan ‚ÄòOwen‚Äô and Tang, Jingyu and Gong, Xinya and Zhou, Zhongyi and Zhang, Shuning and Elvitigala, Don Samitha and Mueller, Florian ‚ÄòFloyd‚Äô and Hu, Wen and Quigley, Aaron J},
  booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  pages     = {1--31},
  year      = {2025}
}


### MLA
Hu, Yongquan ‚ÄòOwen, et al. "Vision-based multimodal interfaces: A survey and taxonomy for enhanced context-aware system design." Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. 2025.


## üß† Abstract
The recent surge in artificial intelligence, particularly in multimodal processing technology, has advanced human-computer interaction, by altering how intelligent systems perceive, understand, and respond to contextual information (i.e., context awareness). Despite such advancements, there is a significant gap in comprehensive reviews examining these advances, especially from a multimodal data perspective, which is crucial for refining system design. This paper addresses a key aspect of this gap by conducting a systematic survey of data modality-driven Vision-based Multimodal Interfaces (VMIs). VMIs are essential for integrating multimodal data, enabling more precise interpretation of user intentions and complex interactions across physical and digital environments. Unlike previous task- or scenario-driven surveys, this study highlights the critical role of the visual modality in processing contextual information and facilitating multimodal interaction. Adopting a design framework moving from the whole to the details and back, it classifies VMIs across dimensions, providing insights for developing effective, context-aware systems.


## üìÇ Dataset (Open Source)
OneDrive Download Link: Coming Soon

‚ö†Ô∏è  Notice
This dataset is only used for academic purposes. If it is used for commercial or other profit-making purposes, legal liability will be pursued.
