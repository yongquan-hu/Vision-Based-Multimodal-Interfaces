
# Vision-Based Multimodal Interfaces (VMIs): A Survey and Taxonomy for Enhanced Context-Aware System Design
The ACM CHI Conference on Human Factors in Computing Systems 2025 (CHI 2025) - full paper: survey material including codebook for the paper

<p align="center">
  <img src="https://github.com/yongquan-hu/Vision-Based-Multimodal-Interfaces/blob/main/VMIs_title.png" width="600" alt="VMIs Title">
</p>

<p align="center">
  <img src="https://github.com/yongquan-hu/Vision-Based-Multimodal-Interfaces/blob/main/teaser.png" width="600" alt="VMIs Teaser">
</p>

# Check Paper:
ACM Library: [https://dl.acm.org/doi/10.1145/3610921](https://dl-acm-org/doi/10.1145/3706598.3714161)
ArXiv: [https://arxiv.org/html/2407.15722v1](https://arxiv.org/abs/2501.13443)

# Cite Paper:
Google scholar link: [https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QqeG8WIAAAAJ&citation_for_view=QqeG8WIAAAAJ:eQOLeE2rZwMC
](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QqeG8WIAAAAJ&sortby=pubdate&citation_for_view=QqeG8WIAAAAJ:YOwf2qJgpHMC)

BibTex:
@inproceedings{hu2025vision,
  title={Vision-based multimodal interfaces: A survey and taxonomy for enhanced context-aware system design},
  author={Hu, Yongquan ‘Owen’ and Tang, Jingyu and Gong, Xinya and Zhou, Zhongyi and Zhang, Shuning and Elvitigala, Don Samitha and Mueller, Florian ‘Floyd’ and Hu, Wen and Quigley, Aaron J},
  booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  pages={1--31},
  year={2025}
}

MLA:
Hu, Yongquan ‘Owen, et al. "Vision-based multimodal interfaces: A survey and taxonomy for enhanced context-aware system design." Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. 2025.


# Abstract
The recent surge in artificial intelligence, particularly in multimodal processing technology, has advanced human-computer interaction, by altering how intelligent systems perceive, understand, and respond to contextual information (i.e., context awareness). Despite such advancements, there is a significant gap in comprehensive reviews examining these advances, especially from a multimodal data perspective, which is crucial for refining system design. This paper addresses a key aspect of this gap by conducting a systematic survey of data modality-driven Vision-based Multimodal Interfaces (VMIs). VMIs are essential for integrating multimodal data, enabling more precise interpretation of user intentions and complex interactions across physical and digital environments. Unlike previous task- or scenario-driven surveys, this study highlights the critical role of the visual modality in processing contextual information and facilitating multimodal interaction. Adopting a design framework moving from the whole to the details and back, it classifies VMIs across dimensions, providing insights for developing effective, context-aware systems.


# Dataset (Open Source)
OneDrive Download Link: Coming Soon


# Notice
This dataset is only used for academic purposes. If it is used for commercial or other profit-making purposes, legal liability will be pursued.
